#!/usr/bin/env python3
"""
æµ‹è¯•å‘é‡ï¼ˆembeddingsï¼‰åŠŸèƒ½
"""
import asyncio
import sys
from pathlib import Path
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent / "backend"))

async def test_embeddings():
    """æµ‹è¯•å‘é‡ç”ŸæˆåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å‘é‡ï¼ˆEmbeddingsï¼‰åŠŸèƒ½")
    print("=" * 50)
    
    # å¯¼å…¥RAGæœåŠ¡
    from backend.services.rag_service import RAGService
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "ä½ å¥½ï¼Œæˆ‘æ˜¯æµ‹è¯•ç”¨æˆ·",
        "ä»Šå¤©å¤©æ°”çœŸå¥½",
        "Hello world, this is a test"
    ]
    
    # æ£€æŸ¥é…ç½®
    use_mock = os.getenv("USE_MOCK_EMBEDDINGS", "true").lower() == "true"
    print(f"USE_MOCK_EMBEDDINGS: {use_mock}")
    
    if not use_mock:
        print("âœ… ä½¿ç”¨çœŸå®çš„Azure OpenAI embeddings")
        endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "")
        deployment = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "")
        print(f"   Endpoint: {endpoint[:50]}...")
        print(f"   Deployment: {deployment}")
    else:
        print("âš ï¸  ä½¿ç”¨Mock embeddings")
    
    print("\næµ‹è¯•å‘é‡ç”Ÿæˆ:")
    
    # åˆå§‹åŒ–RAGæœåŠ¡
    rag_service = RAGService()
    
    # æµ‹è¯•æ¯ä¸ªæ–‡æœ¬
    for i, text in enumerate(test_texts, 1):
        print(f"\n{i}. æ–‡æœ¬: '{text}'")
        try:
            # ç”Ÿæˆå‘é‡
            embedding = await rag_service.generate_embedding(text)
            
            # æ˜¾ç¤ºç»“æœ
            print(f"   å‘é‡é•¿åº¦: {len(embedding)}")
            print(f"   å‰5ä¸ªå€¼: {embedding[:5]}")
            print(f"   ç±»å‹: {type(embedding[0])}")
            
            # éªŒè¯å‘é‡è´¨é‡
            if use_mock:
                # Mockå‘é‡åº”è¯¥åŸºäºæ–‡æœ¬é•¿åº¦
                expected_first = len(text) / 100.0
                if abs(embedding[0] - expected_first) < 0.01:
                    print("   âœ… Mockå‘é‡ç”Ÿæˆæ­£ç¡®")
                else:
                    print("   âŒ Mockå‘é‡ç”Ÿæˆé”™è¯¯")
            else:
                # çœŸå®å‘é‡åº”è¯¥æ˜¯å½’ä¸€åŒ–çš„
                import numpy as np
                norm = np.linalg.norm(embedding)
                print(f"   å‘é‡èŒƒæ•°: {norm:.4f}")
                if 0.9 < norm < 1.1:
                    print("   âœ… å‘é‡å·²å½’ä¸€åŒ–")
                else:
                    print("   âš ï¸  å‘é‡æœªå½’ä¸€åŒ–")
                    
        except Exception as e:
            print(f"   âŒ é”™è¯¯: {type(e).__name__}: {str(e)}")
    
    # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
    if len(test_texts) >= 2:
        print("\n\næµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—:")
        try:
            emb1 = await rag_service.generate_embedding(test_texts[0])
            emb2 = await rag_service.generate_embedding(test_texts[1])
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            import numpy as np
            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
            
            print(f"æ–‡æœ¬1: '{test_texts[0]}'")
            print(f"æ–‡æœ¬2: '{test_texts[1]}'")
            print(f"ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}")
            
            if not use_mock:
                print("\nâœ… Azure OpenAI Embeddings åŠŸèƒ½æ­£å¸¸ï¼")
            else:
                print("\nâš ï¸  å½“å‰ä½¿ç”¨Mockå®ç°ï¼Œå»ºè®®é…ç½®çœŸå®çš„embeddingæœåŠ¡")
                
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
    
    print("\n" + "=" * 50)
    print("æµ‹è¯•å®Œæˆï¼")

async def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    from dotenv import load_dotenv
    load_dotenv("backend/.env")
    
    # åˆå§‹åŒ–æ•°æ®åº“ï¼ˆéœ€è¦ç”¨äºRAGæœåŠ¡ï¼‰
    from backend.core.database import init_db, close_db
    await init_db()
    
    try:
        await test_embeddings()
    finally:
        await close_db()

if __name__ == "__main__":
    asyncio.run(main())